{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b945bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_equation(x1, w1, x2, w2, b):\n",
    "    return x1 * w1 + x2 * w2 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid_function_to_calc_prob(z):\n",
    "    return 1 / (1 + np.exp((-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c00934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_wrt_weights(y, p, x):\n",
    "    loss_wrt_prob = -(y / p - (1 - y) / (1 - p))\n",
    "    sigmoid_derivative = p * (1 - p)\n",
    "    linear_part = x\n",
    "    return loss_wrt_prob * sigmoid_derivative * linear_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283953f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_wrt_bias(p, y):\n",
    "    loss_wrt_prob = -(y / p - (1 - y) / (1 - p))\n",
    "    sigmoid_derivative = p * (1 - p)\n",
    "    return loss_wrt_prob * sigmoid_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc85db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"study hours\" : [6, 7, 8, 9, 5, 6, 2, 0, 11], \"not study hours\" : [4, 7, 3, 8, 5, 5, 2, 5, 1], \"pass or fail\" : [1, 0, 1, 0, 0, 1, 0, 0, 1]},\n",
    "    columns=[\"study hours\", \"not study hours\", \"pass or fail\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression is a supervised machine learning algorithm used for binary classification.\n",
    "# It models the probability of an event occurring, where the predicted probability lies between 0 and 1. \n",
    "# For each data point (row), the model follows these steps:\n",
    "\n",
    "# loss_function = -[ylog(p) + (1-y)log(1-p)] (single data point (each row))\n",
    "# This loss represents the error energy between the predicted probability and the true label.\n",
    "\n",
    "# Since the loss is not directly dependent on the weights, we apply the chain rule:\n",
    "\n",
    "# error energy between probability and truth\n",
    "\n",
    "# derivative:\n",
    "# delta l / delta w (l is not directly dependent on w)\n",
    "\n",
    "# chain rule: delta l / delta w = (delta l / delta p) . (delta p / delta z) . (delta z / delta w)\n",
    "\n",
    "# a. loss wrt p: delta l / delta p = -(y/p - (1-y) / (1-p))\n",
    "# b. sigmoid derivative: delta p / delta z = p(1-p)\n",
    "# c. linear part: delta z / delta w = x\n",
    "\n",
    "\n",
    "# delta l / delta w = (p - y)x\n",
    "# delta l / delta b = (p - y)\n",
    "\n",
    "# update rule(learning) = w new = w - eta(p-y)x\n",
    "# b new = b - eta(p-y)\n",
    "\n",
    "# eta is learning rate\n",
    "# eta : if it is larger (minimum miss, loss oscillate,  learning fail)\n",
    "# eta : if it smaller (learning slow)\n",
    "\n",
    "\n",
    "# eta: no fixed value\n",
    "# scaled data = 0.01, 0.05, 0.1\n",
    "# unscaled data = very small etaâ€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e81174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_learning():\n",
    "    w1, w2, b = 0, 0 ,0\n",
    "    epoch = 100\n",
    "    eta = 0.01\n",
    "\n",
    "    for i in range(epoch):\n",
    "        for idx, raw in df.iterrows():\n",
    "            gradients = None\n",
    "            x1, x2, y = raw.to_list()\n",
    "            z = calculate_model_equation(x1, w1, x2, w2, b)\n",
    "            p = sigmoid_function_to_calc_prob(z)\n",
    "            p = min(max(p, 1e-7), 1 - 1e-7)\n",
    "            # error_energy = p - y\n",
    "\n",
    "            gradients = calculate_loss_wrt_weights(y = y, p = p, x = x1)\n",
    "            w1 = w1 - eta * gradients\n",
    "            gradients = None\n",
    "            gradients = calculate_loss_wrt_weights(y = y, p = p, x = x2)\n",
    "            w2 = w2 - eta * gradients\n",
    "            gradients = None\n",
    "            gradients = calculate_loss_wrt_bias(p = p, y = y)\n",
    "            b = b - eta * gradients\n",
    "\n",
    "            # print(f\"weight 1: {round(w1, 2)}\\tweight 2: {round(w2, 2)}\\tbias : {round(b, 2)}\")\n",
    "\n",
    "    print(f\"weight 1: {round(w1, 2)}\\tweight 2: {round(w2, 2)}\\tbias : {round(b, 2)}\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing():\n",
    "    x1, x2 = 2, 5\n",
    "    z = calculate_model_equation(x1=x1, w1=0.97, x2=x2, w2=-1.21, b=-0.05)\n",
    "    p = sigmoid_function_to_calc_prob(z)\n",
    "    print(f\"probability: {round(p, 2)}\")\n",
    "model_testing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
