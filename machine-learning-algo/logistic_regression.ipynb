{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10b945bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_equation(x1 = None, w1 = None, x2 = None, w2 = None, b = None, list_ = None, weights = None):\n",
    "    if list_ and weights is not None:\n",
    "        res = 0\n",
    "        for i in range(len(list_)):\n",
    "            ele1 = list_[i]\n",
    "            ele2 = weights[i]\n",
    "            res += ele1 * ele2\n",
    "        \n",
    "        return res + b\n",
    "    return x1 * w1 + x2 * w2 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2465042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid_function_to_calc_prob(z):\n",
    "    return 1 / (1 + np.exp((-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "42c00934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_wrt_weights(y, p, x):\n",
    "    loss_wrt_prob = -(y / p - (1 - y) / (1 - p))\n",
    "    sigmoid_derivative = p * (1 - p)\n",
    "    linear_part = x\n",
    "    return loss_wrt_prob * sigmoid_derivative * linear_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "283953f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_wrt_bias(p, y):\n",
    "    loss_wrt_prob = -(y / p - (1 - y) / (1 - p))\n",
    "    sigmoid_derivative = p * (1 - p)\n",
    "    return loss_wrt_prob * sigmoid_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc85db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"study hours\" : [6, 7, 8, 9, 5, 6, 2, 0, 11], \"not study hours\" : [4, 7, 3, 8, 5, 5, 2, 5, 1], \"pass or fail\" : [1, 0, 1, 0, 0, 1, 0, 0, 1]},\n",
    "    columns=[\"study hours\", \"not study hours\", \"pass or fail\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression is a supervised machine learning algorithm used for binary classification.\n",
    "# It models the probability of an event occurring, where the predicted probability lies between 0 and 1. \n",
    "# For each data point (row), the model follows these steps:\n",
    "\n",
    "# loss_function = -[ylog(p) + (1-y)log(1-p)] (single data point (each row))\n",
    "# This loss represents the error energy between the predicted probability and the true label.\n",
    "\n",
    "# Since the loss is not directly dependent on the weights, we apply the chain rule:\n",
    "\n",
    "# error energy between probability and truth\n",
    "\n",
    "# derivative:\n",
    "# delta l / delta w (l is not directly dependent on w)\n",
    "\n",
    "# chain rule: delta l / delta w = (delta l / delta p) . (delta p / delta z) . (delta z / delta w)\n",
    "\n",
    "# a. loss wrt p: delta l / delta p = -(y/p - (1-y) / (1-p))\n",
    "# b. sigmoid derivative: delta p / delta z = p(1-p)\n",
    "# c. linear part: delta z / delta w = x\n",
    "\n",
    "\n",
    "# delta l / delta w = (p - y)x\n",
    "# delta l / delta b = (p - y)\n",
    "\n",
    "# update rule(learning) = w new = w - eta(p-y)x\n",
    "# b new = b - eta(p-y)\n",
    "\n",
    "# eta is learning rate\n",
    "# eta : if it is larger (minimum miss, loss oscillate,  learning fail)\n",
    "# eta : if it smaller (learning slow)\n",
    "\n",
    "\n",
    "# eta: no fixed value\n",
    "# scaled data = 0.01, 0.05, 0.1\n",
    "# unscaled data = very small etaâ€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e81174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_learning():\n",
    "    w1, w2, b = 0, 0 ,0\n",
    "    epoch = 10000\n",
    "    eta = 0.01\n",
    "\n",
    "    for i in range(epoch):\n",
    "        for idx, raw in df.iterrows():\n",
    "            gradients = None\n",
    "            x1, x2, y = raw.to_list()\n",
    "            z = calculate_model_equation(x1, w1, x2, w2, b)\n",
    "            p = sigmoid_function_to_calc_prob(z)\n",
    "            p = min(max(p, 1e-7), 1 - 1e-7)\n",
    "            # error_energy = p - y\n",
    "\n",
    "            gradients = calculate_loss_wrt_weights(y = y, p = p, x = x1)\n",
    "            w1 = w1 - eta * gradients\n",
    "            gradients = None\n",
    "            gradients = calculate_loss_wrt_weights(y = y, p = p, x = x2)\n",
    "            w2 = w2 - eta * gradients\n",
    "            gradients = None\n",
    "            gradients = calculate_loss_wrt_bias(p = p, y = y)\n",
    "            b = b - eta * gradients\n",
    "\n",
    "            # print(f\"weight 1: {round(w1, 2)}\\tweight 2: {round(w2, 2)}\\tbias : {round(b, 2)}\")\n",
    "\n",
    "    print(f\"weight 1: {round(w1, 2)}\\tweight 2: {round(w2, 2)}\\tbias : {round(b, 2)}\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing():\n",
    "    x1, x2 = 5, 5\n",
    "    z = calculate_model_equation(x1=x1, w1=8.46, x2=x2, w2=-9.82, b=0.11)\n",
    "    p = sigmoid_function_to_calc_prob(z)\n",
    "    print(f\"probability: {round(p, 2)}\")\n",
    "model_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa45f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "kagglehub.dataset_download(\"neurocipher/heartdisease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7eabd8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 14)\n",
      "['Presence' 'Absence']\n",
      "(270,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(270, 11)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "if os.path.exists(r\"C:\\Users\\madha\\.cache\\kagglehub\\datasets\\neurocipher\\heartdisease\\versions\\1\\Heart_Disease_Prediction.csv\"): \n",
    "    read_csv = pd.read_csv(r\"C:\\Users\\madha\\.cache\\kagglehub\\datasets\\neurocipher\\heartdisease\\versions\\1\\Heart_Disease_Prediction.csv\")\n",
    "\n",
    "# print(read_csv.head())\n",
    "print(read_csv.shape)\n",
    "\n",
    "columns_name = read_csv.columns\n",
    "# print(columns_name)\n",
    "\n",
    "labels = read_csv[\"Heart Disease\"]\n",
    "print(labels.unique())\n",
    "print(labels.shape)\n",
    "\n",
    "read_csv.drop(columns=[\"Age\", \"Sex\", \"Heart Disease\"], axis=1, inplace=True)\n",
    "features = read_csv\n",
    "read_csv.shape\n",
    "\n",
    "# read_csv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db067a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Chest pain type       BP  Cholesterol  FBS over 120  EKG results    Max HR  \\\n",
      "0         0.869313 -0.07527     1.399613     -0.416256     0.979844 -1.755947   \n",
      "\n",
      "   Exercise angina  ST depression  Slope of ST  Number of vessels fluro  \\\n",
      "0        -0.699923       1.178823     0.675165                 2.468099   \n",
      "\n",
      "   Thallium  \n",
      "0 -0.874083  \n",
      "unique labels: ['Presence' 'Absence']\n",
      "after encoding classes_: ['Absence' 'Presence']\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "features = (features - features.mean()) / features.std()\n",
    "print(features.head(1))\n",
    "\n",
    "print(f\"unique labels: {labels.unique()}\")\n",
    "labels = encoder.fit_transform(labels)\n",
    "print(f\"after encoding classes_: {encoder.classes_}\")\n",
    "labels = pd.Series(labels)\n",
    "print(labels.unique())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, train_size=.80, test_size=.20, random_state=42, shuffle=True)\n",
    "# x_train = x_train.reset_index(drop=True)\n",
    "# x_test = x_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0e75f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [np.float64(0.6068708639317488), np.float64(0.4752463360602253), np.float64(-0.03395310918851872), np.float64(-0.3735583669367338), np.float64(0.1835417202181511), np.float64(-0.2119323091907539), np.float64(0.35447341452595854), np.float64(0.6295781373060503), np.float64(0.3362802448045449), np.float64(0.7628367881163033), np.float64(0.8545847067682114)]\n",
      "bias: -0.25828347770108123\n"
     ]
    }
   ],
   "source": [
    "epoch = 10000\n",
    "\n",
    "weights = [0] * read_csv.shape[1]\n",
    "bias = 0\n",
    "learning_rate = 0.05\n",
    "\n",
    "for e in range(epoch):\n",
    "    for idx, row in x_train.iterrows():\n",
    "        list_ = row.to_list()\n",
    "        z = calculate_model_equation(b=bias, list_=list_, weights=weights)\n",
    "        p = sigmoid_function_to_calc_prob(z = z)\n",
    "        p = min(max(p, 1e-7), 1 - 1e-7)\n",
    "\n",
    "        gradients = None\n",
    "        for i in range(len(list_)):\n",
    "            gradients = calculate_loss_wrt_weights(y = y_train[idx], p = p, x = list_[i])\n",
    "            weights[i] = weights[i] - learning_rate * gradients\n",
    "        \n",
    "        gradients = calculate_loss_wrt_bias(p = p, y = y_train[idx])\n",
    "        bias = bias - learning_rate * gradients\n",
    "    x_train = x_train.sample(frac=1)\n",
    "print(f\"weights: {weights}\\nbias: {bias}\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8684ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TT\": 17,\n",
      "    \"TF\": 4,\n",
      "    \"FT\": 1,\n",
      "    \"FF\": 32\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = {\"TT\": 0, \"TF\": 0, \"FT\": 0, \"FF\": 0}\n",
    "for idx, row in x_test.iterrows():\n",
    "    list_ = row.to_list()\n",
    "    z = calculate_model_equation(b=bias, list_=list_, weights=weights)\n",
    "    p = sigmoid_function_to_calc_prob(z=z)\n",
    "    truth = y_test[idx]\n",
    "    if truth == 1:\n",
    "        if p > 0.5:\n",
    "            confusion_matrix[\"TT\"] += 1\n",
    "        elif p <= 0.5:\n",
    "            confusion_matrix[\"TF\"] += 1\n",
    "    elif truth == 0:\n",
    "        if p > 0.5:\n",
    "            confusion_matrix[\"FT\"] += 1\n",
    "        elif p <= 0.5:\n",
    "            confusion_matrix[\"FF\"] += 1\n",
    "\n",
    "import json\n",
    "print(json.dumps(confusion_matrix, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ebef8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy: 0.91\n",
      "model precision: 0.94\n",
      "model recall: 0.81\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = 0\n",
    "\n",
    "count_data = sum(value for _, value in confusion_matrix.items())\n",
    "correct_predictions = confusion_matrix[\"TT\"] + confusion_matrix[\"FF\"]\n",
    "\n",
    "model_accuracy = correct_predictions / count_data\n",
    "print(f\"model accuracy: {round(model_accuracy, 2)}\")\n",
    "\n",
    "model_precision = 0\n",
    "\n",
    "truetrue_case = confusion_matrix[\"TT\"]\n",
    "falsetrue_case = confusion_matrix[\"FT\"]\n",
    "\n",
    "model_precision = truetrue_case / (truetrue_case + falsetrue_case)\n",
    "print(f\"model precision: {round(model_precision, 2)}\")\n",
    "\n",
    "model_recall = 0\n",
    "\n",
    "truetrue_case = confusion_matrix[\"TT\"]\n",
    "truefalse_case = confusion_matrix[\"TF\"]\n",
    "\n",
    "model_recall = truetrue_case / (truetrue_case + truefalse_case)\n",
    "print(f\"model recall: {round(model_recall, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae33ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b496a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
